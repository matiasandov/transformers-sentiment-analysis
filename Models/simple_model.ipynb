{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting flair\n",
      "  Using cached flair-0.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting boto3>=1.20.27 (from flair)\n",
      "  Downloading boto3-1.34.57-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting bpemb>=0.3.2 (from flair)\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting conllu>=4.0 (from flair)\n",
      "  Using cached conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair)\n",
      "  Using cached ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting gdown>=4.4.0 (from flair)\n",
      "  Using cached gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting gensim>=4.2.0 (from flair)\n",
      "  Downloading gensim-4.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting huggingface-hub>=0.10.0 (from flair)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting janome>=0.4.2 (from flair)\n",
      "  Using cached Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langdetect>=1.0.9 (from flair)\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.8.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from flair) (5.1.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from flair) (3.8.1)\n",
      "Collecting more-itertools>=8.13.0 (from flair)\n",
      "  Using cached more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting mpld3>=0.3 (from flair)\n",
      "  Using cached mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair)\n",
      "  Using cached pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from flair) (2.8.2)\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
      "  Using cached pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting regex>=2022.1.18 (from flair)\n",
      "  Downloading regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.2 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from flair) (1.3.2)\n",
      "Collecting segtok>=1.5.11 (from flair)\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair)\n",
      "  Using cached sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tabulate>=0.8.10 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from flair) (0.9.0)\n",
      "Collecting torch!=1.8,>=1.5.0 (from flair)\n",
      "  Downloading torch-2.2.1-cp39-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from flair) (4.66.1)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
      "  Using cached transformer_smaller_training_vocab-0.3.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting transformers<5.0.0,>=4.18.0 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<2.0.0,>=1.0.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from flair) (1.26.18)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair)\n",
      "  Using cached Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting semver<4.0.0,>=3.0.0 (from flair)\n",
      "  Using cached semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.57 (from boto3>=1.20.27->flair)\n",
      "  Downloading botocore-1.34.57-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair)\n",
      "  Downloading s3transfer-0.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from bpemb>=0.3.2->flair) (1.26.1)\n",
      "Requirement already satisfied: requests in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
      "Collecting sentencepiece (from bpemb>=0.3.2->flair)\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy>=6.1.0->flair)\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from gdown>=4.4.0->flair) (4.12.2)\n",
      "Collecting filelock (from gdown>=4.4.0->flair)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from gensim>=4.2.0->flair) (1.11.3)\n",
      "Collecting smart-open>=1.8.1 (from gensim>=4.2.0->flair)\n",
      "  Downloading smart_open-7.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.10.0->flair) (2023.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.10.0->flair) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.10.0->flair) (23.2)\n",
      "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from langdetect>=1.0.9->flair) (1.15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from matplotlib>=2.2.3->flair) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from matplotlib>=2.2.3->flair) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from matplotlib>=2.2.3->flair) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from matplotlib>=2.2.3->flair) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from matplotlib>=2.2.3->flair) (6.1.1)\n",
      "Requirement already satisfied: jinja2 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from mpld3>=0.3->flair) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.0.2->flair) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.0.2->flair) (3.2.0)\n",
      "Collecting sympy (from torch!=1.8,>=1.5.0->flair)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch!=1.8,>=1.5.0->flair)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting sentencepiece (from bpemb>=0.3.2->flair)\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.18.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.18.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: protobuf in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (4.23.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.2.3->flair) (3.17.0)\n",
      "Collecting accelerate>=0.21.0 (from transformers[torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair)\n",
      "  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from jinja2->mpld3>=0.3->flair) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from requests->bpemb>=0.3.2->flair) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from requests->bpemb>=0.3.2->flair) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from requests->bpemb>=0.3.2->flair) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
      "Collecting mpmath>=0.19 (from sympy->torch!=1.8,>=1.5.0->flair)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: psutil in /Users/matiasmendez/Library/Python/3.9/lib/python/site-packages (from accelerate>=0.21.0->transformers[torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.6)\n",
      "Using cached flair-0.13.1-py3-none-any.whl (388 kB)\n",
      "Downloading boto3-1.34.57-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Using cached conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "Using cached gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading gensim-4.3.2-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "Using cached more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "Using cached pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Downloading regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Using cached semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading torch-2.2.1-cp39-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Downloading botocore-1.34.57-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp39-cp39-macosx_11_0_arm64.whl (394 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.1.99-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp39-cp39-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m606.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: langdetect, pptree, sqlitedict\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=4fa77d77da2f52e4f91ddeee434bb14223637200c3078f7809d37ce83650b8a3\n",
      "  Stored in directory: /Users/matiasmendez/Library/Caches/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for pptree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=9f45f31514f060a4b8b74d1b676cb7f3acd21986f22e767c0735580c75fedc5e\n",
      "  Stored in directory: /Users/matiasmendez/Library/Caches/pip/wheels/52/0e/51/514e690004ea9713bc3fdb678d5e2768fcc597d0c3b6a3abd2\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=f7443fc14c2416135c2ed7799bc191b3d81e575e84c139e687adf8b3366bbe5e\n",
      "  Stored in directory: /Users/matiasmendez/Library/Caches/pip/wheels/f6/48/c4/942f7a1d556fddd2348cb9ac262f251873dfd8a39afec5678e\n",
      "Successfully built langdetect pptree sqlitedict\n",
      "Installing collected packages: wcwidth, sqlitedict, sentencepiece, pptree, mpmath, janome, sympy, smart-open, semver, safetensors, regex, networkx, more-itertools, langdetect, jmespath, ftfy, filelock, deprecated, conllu, wikipedia-api, torch, segtok, huggingface-hub, gensim, botocore, tokenizers, s3transfer, pytorch-revgrad, mpld3, gdown, bpemb, accelerate, transformers, boto3, transformer-smaller-training-vocab, flair\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.8\n",
      "    Uninstalling wcwidth-0.2.8:\n",
      "      Successfully uninstalled wcwidth-0.2.8\n",
      "Successfully installed accelerate-0.27.2 boto3-1.34.57 botocore-1.34.57 bpemb-0.3.4 conllu-4.5.3 deprecated-1.2.14 filelock-3.13.1 flair-0.13.1 ftfy-6.1.3 gdown-5.1.0 gensim-4.3.2 huggingface-hub-0.21.4 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 more-itertools-10.2.0 mpld3-0.5.10 mpmath-1.3.0 networkx-3.2.1 pptree-3.1 pytorch-revgrad-0.2.0 regex-2023.12.25 s3transfer-0.10.0 safetensors-0.4.2 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 smart-open-7.0.1 sqlitedict-2.1.0 sympy-1.12 tokenizers-0.15.2 torch-2.2.1 transformer-smaller-training-vocab-0.3.3 transformers-4.38.2 wcwidth-0.2.13 wikipedia-api-0.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair uses PyTorch/TensorFlow in under the hood, so it's essential that you also have one of the two libraries (or both) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiasmendez/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import flair\n",
    "#english language model for sentiment analysis\n",
    "model = flair.models.TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to tokenize input text. For this we use the Flair Sentence object, which we initialize by passing our text into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[7]: \"I like you. I love you\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I like you. I love you\"  # we are expecting a confidently positive sentiment here\n",
    "\n",
    "sentence = flair.data.Sentence(text)\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[7]: \"I like you. I love you\" â†’ POSITIVE (0.9933)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sentence)\n",
    "# The predict method doesn't output our prediction, instead the predictions are added to our sentence:\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence[7]: \"I like you. I love you\"'/'POSITIVE' (0.9933)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.get_labels()[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with `nft` related posts extracted from the crawler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[52]: \"ğŸ“Š  Fintech Focus ğŸ”  ğŸ“¢  NFTs - Are Non-Fungible Tokens The Next Big Thing  âŒ› or Just Hype? Read my latest post  ğŸ‘‡ to learn more and share your thoughts/experiences/ideas! #fintech #friday #nfts #nftcommunity #blockchain #cryptocurrency #cryptoexchange #art\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = 'ğŸ“Š  Fintech Focus ğŸ”  ğŸ“¢  NFTs - Are Non-Fungible Tokens The Next Big Thing  âŒ›ï¸ or Just Hype? Read my latest post  ğŸ‘‡ to learn more and share your thoughts/experiences/ideas! #fintech #friday #nfts #nftcommunity #blockchain #cryptocurrency #cryptoexchange #art'\n",
    "sentence = flair.data.Sentence(post)\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence[52]: \"ğŸ“Š  Fintech Focus ğŸ”  ğŸ“¢  NFTs - Are Non-Fungible Tokens The Next Big Thing  âŒ› or Just Hype? Read my latest post  ğŸ‘‡ to learn more and share your thoughts/experiences/ideas! #fintech #friday #nfts #nftcommunity #blockchain #cryptocurrency #cryptoexchange #art\"'/'POSITIVE' (0.8608)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sentence)\n",
    "sentence.get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post content could be more considered as negative than positive since it is questioning the NFTs hype, or even neutral but not at all positive. So we can conclude the model is probably not going to work just like this. We should built our own or try to adjust it.\n",
    "\n",
    "I will experiment to see if taking out the hasthags improve the true content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence[52]: \"ğŸ“Š  Fintech Focus ğŸ”  ğŸ“¢  NFTs - Are Non-Fungible Tokens The Next Big Thing  âŒ› or Just Hype? Read my latest post  ğŸ‘‡ to learn more and share your thoughts/experiences/ideas! #fintech #friday #nfts #nftcommunity #blockchain #cryptocurrency #cryptoexchange #art\"'/'POSITIVE' (0.8608)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = 'ğŸ“Š  Fintech Focus ğŸ”  ğŸ“¢  NFTs - Are Non-Fungible Tokens The Next Big Thing  âŒ›ï¸ or Just Hype? Read my latest post  ğŸ‘‡ to learn more and share your thoughts/experiences/ideas! #fintech #friday #nfts #nftcommunity #blockchain #cryptocurrency #cryptoexchange #art'\n",
    "sentence = flair.data.Sentence(post)\n",
    "model.predict(sentence)\n",
    "sentence.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    # Use a regular expression to find and remove hashtags\n",
    "    #replaces every hashtag with a ''\n",
    "    cleaned_text = re.sub(r'#\\w+', '', text)\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence[36]: \"ğŸ“Š  Fintech Focus ğŸ”  ğŸ“¢  NFTs - Are Non-Fungible Tokens The Next Big Thing  âŒ› or Just Hype? Read my latest post  ğŸ‘‡ to learn more and share your thoughts/experiences/ideas!\"'/'NEGATIVE' (0.9315)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = 'ğŸ“Š  Fintech Focus ğŸ”  ğŸ“¢  NFTs - Are Non-Fungible Tokens The Next Big Thing  âŒ›ï¸ or Just Hype? Read my latest post  ğŸ‘‡ to learn more and share your thoughts/experiences/ideas! #fintech #friday #nfts #nftcommunity #blockchain #cryptocurrency #cryptoexchange #art'\n",
    "#\n",
    "post = remove_hashtags(post)\n",
    "sentence = flair.data.Sentence(post)\n",
    "model.predict(sentence)\n",
    "sentence.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.get_labels()[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that removing the hashtags definitely improved the prediction. So now, let's test it with more post contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def txt_to_list(file_path):\n",
    "\n",
    "    lines_list = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Append each line as a string to the list (remove newline characters)\n",
    "                lines_list.append(line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return lines_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Last week I had the pleasure to NFT Paris by the hand of Arianee . Witnessing the transformative power of Web3 and NFTs in reshaping the concept of ownership, particularly in relation to our personal data, left me inspired and eager to delve deeper into the evolving landscape of decentralized technologies. The event sparked valuable insights, and I'm excited to continue navigating the dynamic realm of blockchain innovations. #NFTParis #Web3 #OwnershipRevolution\",\n",
       " 'Hello everyone Today, my friends Asma Ghamacha , Hermes Yan NTJAM NDJENG , Harold Geumtcheng , Aloys Aymrick Nzooh , Bryan Fozame and I had the chance to be part of the NFT Paris conference thanks to our school aivancity School for Technology, Business & Society Paris-Cachan where we learned a wealth of new information about blockchain, metaverse, web3 and its use cases across various industries such as finance, gaming, luxury, and more. During this enI had the privilege to engage in discussions with numerous brilliant web3 developers and CEOs from companies like Maxence Perray from Nomiks , Victor Briere from Arianee , Ubisoft , Louis Vuitton , and many others. These conversations provided me with deeper insights into this innovative technology and its intersection with data science, particularly in terms of transparency and tokenization on the blockchain. #DataScience #Blockchain']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'descriptions.txt'\n",
    "\n",
    "posts = txt_to_list(path)\n",
    "posts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Last week I had the pleasure to NFT Paris by the hand of Arianee . Witnessing the transformative power of Web3 and NFTs in reshaping the concept of ownership, particularly in relation to our personal data, left me inspired and eager to delve deeper into the evolving landscape of decentralized technologies. The event sparked valuable insights, and I'm excited to continue navigating the dynamic realm of blockchain innovations.\",\n",
       " 'Hello everyone Today, my friends Asma Ghamacha , Hermes Yan NTJAM NDJENG , Harold Geumtcheng , Aloys Aymrick Nzooh , Bryan Fozame and I had the chance to be part of the NFT Paris conference thanks to our school aivancity School for Technology, Business & Society Paris-Cachan where we learned a wealth of new information about blockchain, metaverse, web3 and its use cases across various industries such as finance, gaming, luxury, and more. During this enI had the privilege to engage in discussions with numerous brilliant web3 developers and CEOs from companies like Maxence Perray from Nomiks , Victor Briere from Arianee , Ubisoft , Louis Vuitton , and many others. These conversations provided me with deeper insights into this innovative technology and its intersection with data science, particularly in terms of transparency and tokenization on the blockchain.',\n",
       " 'The digital world continues to intersect with traditional art forms as online trading platform Robinhood partners with Notable.art to bring prominent artist Hunt Slonemâ€™s work to a wider audience through the use of Non-Fungible Tokens (NFTs) Sign up on our website to automatically enter our monthly prize raffle https://lnkd.in/dMcKcrpf',\n",
       " \"Canâ€™t wait to welcome everyone in Paris this week for NFT Paris 2024! ğŸš€ Shout to Alexandre Tsydenkov and the team at NFT Paris who have done an awesome job creating and curating one of the most relevant B2B and B2C web3 events in the world. Arianee & I will be present throughout the week at side-events, panels and of course, the main event on the 23rd and 24th with demos and a workshop. This year, Arianee will be taking a different approach with new activations: from taking over the VIP lounge, where we will be exhibiting all the brands building on our technologyâ€¦ from BREITLING to Panerai , Moncler , and many more, showcasing our tokenized digital product passports. Join me on stage on February 23rd for: 1ï¸âƒ£ The opening keynote â€œFrom Hype to Purpose - Redefining NFTs for the next billion usersâ€, at 10:15am CET 2ï¸âƒ£ A panel with ian rogers (Chief Experience Officer, Ledger ) and Gmoney, on â€œUnleashing the Potential of the Digital Luxury Marketâ€ at 11:00am CET Our fellow team members & esteemed partners will also be on stage: 3ï¸âƒ£ Delphine EddÃ© , our CMO, will be moderating a panel on â€œDigital Product Passports for Physical Goods: From Post Purchase Engagement to Circular Business Models with Eva Assayag (Head of IS & Organization Projects, Panerai ), Adrian Corsin (Managing Director, MUGLER ), Michele Lo Forte (Global Head of E-Commerce and Digital Customer Engagement, BREITLING ) on Friday 23rd at 12:40am CET 4ï¸âƒ£ Alexandre Mare will be joining Fabien Aufrechter (Head of Web3, Vivendi ), Sandy Carter (COO, Unstoppable Domains ), moderated by Farokh Sarmad (Rug Media) to discuss â€œOnboarding the Next Billions Users into Web3: Use Cases, Challenges, Opportunitiesâ€ on Saturday 24th at 12:40am CET 5ï¸âƒ£ Our Lead Developer Maxime Vaullerin will host a workshop on â€œEnhance Digital Product Passport Utilities with Interoperabilityâ€ on Saturday 24th at 3:00pm CET Last but not least, weâ€™ll be co-hosting the Speakers Dinner on Thursday, 22nd and a lunch with Polygon Labs on the 23rd in the VIP Lounge. If you have some time, go check out MusÃ©e d'Orsay , where my dear friend { Agoria } , an amazing musician and NFT artist, has opened an exhibition with two extraordinary works created specifically for the museum. Go check it out! I might be organizing a little private tour, so DM if you are interested ;-) Itâ€™s going to be a crazy week, looking forward to seeing you all. Thanks to the amazing Arianee team for making it happen. Donâ€™t hesitate to drop me a note if youâ€™re interested in setting up a meeting, want to attend any of the side events or need any food tips for Paris Click here to get the full Arianee agenda: https://lnkd.in/euJePd_D ğŸ“… Save the Date: February 23-24, 2024 ğŸ“ Location: Grand Palais Ã‰phÃ©mÃ¨re, Paris\",\n",
       " \"DualMint is set to launch Toji NFT, Japanese sake tokenized on the blockchain on the 2nd March,2024. It's in collaboration with the 200 year old sake producers, Daimon Brewery. This partnership is a big deal because it showcases real world assets tokenization apart from the existing real estate tokenization and financial product tokenization. Want to be a part of the launch? Read more in the latest newsletter.\",\n",
       " \"This Week on RWA Insights ğŸš€ ğŸ“Œ Dive into how Day by Day leverages RWAs and AI to redefine insurance, ğŸ“Œ Get ready for DualMint's Toji NFT launch on March 2nd, 2024. ğŸ“Œ Explore our featured blog post on revolutionizing the commodities market. Don't miss out on these groundbreaking insights!\",\n",
       " 'Join me Saturday, February 24th at NFT Paris where I will be speaking alongside Fabien Aufrechter (Head of Web3, Vivendi), Sandy Carter (COO, Unstoppable Domains), moderated by Farokh Sarmad (Rug Media) to discuss â€œOnboarding the Next Billions Users into Web3: Use cases, Challenges, Opportunitiesâ€. Discover how Arianeeâ€™s digital product passports can unlock a new circular economy. Arianee will also have a massive presence at NFT Paris, full of panels, workshops & demo. See the full agenda below ğŸ‘‡ ğŸ“… Save the Date: February 23-24, 2024 ğŸ“ Location: Grand Palais Ã‰phÃ©mÃ¨re, Paris',\n",
       " 'NFT Paris 2024 starts in exactly one week! Besides taking over the VIP Lounge, we have a couple other things up our sleeveâ€¦ Catch out our keynote, panels, workshop and look out for the Arianee team (hint: we might be wearing something pink ğŸ˜‰), especially in the Builder Zone booth .. Check out the details below, save and share this post, and see you next week! Day 1: February 23rd Opening keynote: From Hype to Purpose: Redefining NFTs for The Next Billion Users ğŸ¤ Pierre-Nicolas Hurstel (CEO & Co-Founder, Arianee) ğŸ—“ï¸ 10:15am - 10:30am ğŸ“ Main Stage Panel 1 : Digital Product Passports for Physical Goods: From Post Purchase Engagement to Circular Business Models ğŸ¤ Eva Assayag (Head of IS & Organization Projects, Panerai), Adrian Corsin (Managing Director, MUGLER ), Michele Lo Forte (Global Head of E-Commerce and Digital Customer Engagement, BREITLING ) | Moderated by Delphine EddÃ© (CMO, Arianee) ğŸ—“ï¸12:40am - 1:10pm ğŸ“Main Stage Panel 2: 2034: Unleashing the Potential of the Digital Luxury Market ğŸ¤ ian rogers (Chief Experience Officer, Ledger ), Gmoney and Pierre-Nicolas (CEO & Co-Founder, Arianee) | Moderated by Amanda Cassatt \\u200b\\u200b(CEO & Founder, Serotonin ) ğŸ—“ï¸11:00am - 11:30am ğŸ“Main Stage Day 2: February 24th Panel 3: Onboarding The Next Billions Users into Web3: Use Cases, Challenges, Opportunities ğŸ¤ Fabien Aufrechter (Head of Web3, Vivendi ), Sandy Carter (COO, Unstoppable Domains ) and Alexandre Mare (COO, Arianee) | Moderated by Farokh Sarmad , Rug Radio ) ğŸ—“ï¸12:40am - 1:10pm ğŸ“Main Stage Workshop: Enhance Digital Product Passport Utilities with Interoperability - The Arianee Case ğŸ¤ Maxime Vaullerin (Lead Developer, Arianee) ğŸ—“ï¸3:00pm - 3:35pm ğŸ“Eiffel Stage ğŸ‘€ Look out for more announcements next week! Book a meeting with us in advance: https://lnkd.in/guSVreC4 Still havenâ€™t secured your tickets? Follow the link below and get a special discount with the code PN15 https://lnkd.in/dn8D3xHR Discover more on our digital product passport solutions: https://lnkd.in/e78re2hD ğŸ“… Save the Date: February 23-24, 2024 ğŸ“ Location: Grand Palais Ã‰phÃ©mÃ¨re, Paris See you there!',\n",
       " 'Hello everyone Today, I had the chance to be part of the NFT Paris conference where I learned a wealth of new information about blockchain and its use cases across various industries such as finance, gaming, luxury, and more. I had the privilege to engage in discussions with numerous brilliant web3 developers and CEOs from companies like Maxence Perray from  , Victor Briere from  ,  ,  ,  , and many others. These conversations provided me with deeper insights into this innovative technology and its intersection with data science, particularly in terms of transparency and tokenization on the blockchain.',\n",
       " 'â€œComment acheter les NFT de â€˜Eyes of Humanityâ€™ et en tirer profitâ€',\n",
       " 'A guide to   and a snapshot of data trends.',\n",
       " 'For a long time I imagined, this term or there was always something. Well, and now, regardless of the circumstances, I said that waiting is enough and that nothing more needs to be done for this to happen. Thus, I publicly presented my alter ego \"The Knight with the Rose\" ( https://lnkd.in/gDWS9mas ), my music group MonteCristo ( https://lnkd.in/g_p2V9KQ ), and the first NFT of my book! and combined it all into the NIGHT WITHOUT GAME event. With this, exactly what I have been doing all my life was born and made sense. Art, lecture, conversation, music... the masks have fallen, the games are gone. We are the only ones left who watch with our hearts!',\n",
       " \"Brands can now create digital narratives where customers can actively participate, adding an immersive dimension to their marketing efforts. Imagine launching a product line where each item is accompanied by an NFT, each telling a different story of the brand's heritage or the product's journey. This approach not only enriches the customer experience but also fosters a deeper connection with the brand.\",\n",
       " 'neutral',\n",
       " 'A bit more than 17 300 entries over 2 days - NFT Paris was a blast !! Particularly proud of the attendance of major companies in the luxury, sport, gaming, finance, art sector and their support this year. Even Tesla was here ! NFTs are evolving, the sector is becoming more mature - From Hype to purpose introduction keynote from Pierre-Nicolas Hurstel is the defining theme of this edition. Once hype is gone, what remains ? 1 - A brilliant community & culture. 2 - Real use cases that finally appear. Slowly, then all at once - let the consolidation do its work.',\n",
       " 'A guide to Bored Ape Yacht Club  and a snapshot of data trends.',\n",
       " 'A SaaS platform on top of the Arianee Protocol - to distribute gasless NFTs with special features and engage customers securely, anonymously and at scale!',\n",
       " \"NFTs are revolutionizing the way we think about ownership and value in the digital world in ways that are deeper and more meaningful than having a customerâ€™s email address. But, as with any nascent technology, there's a lot of complexity to navigate. That's where the NFT Management Platform (NMP) comes in to make web3 more accessible to brands. In this article, we'll explore why we created the NMP and the future we envision for it.\",\n",
       " 'Is this the best way to capitalize on a Web3 conference ? Some context : At the end of last year we decided with the BGA team to double down  on our effort to provide an opportunity for BGA members and Web3 Gaming enthusiasts to network and learn from each other. We want  to be the embodiment of this new comitment. During NFT Paris, we were very happy to see that the different verticals of BGAConnects made sense for every atendee : ğŸ¤ Learn from the experience of those who are innovating. We were very fortunate to have Yat Siu , Nicolas Gilot among many brilliant entrepreneurs in our 5 panels. ğŸ† Give an opportunity for start ups to meet potential investors ğŸ•¹ Just... Play some games and meet the visionaries who are behing. One last note : Always try to contribute to other events based on the expertise you could provide as a network of professionals in a specific ecosystem. Super happy that our members had the chance to play a role in some of the best side events of NFT Paris. See our takeaways here : https://lnkd.in/eGHhc336',\n",
       " 'In just two days, YSL BeautÃ© will reveal their latest NFT chapter, \"The Night Is Ours\". As we wait in anticipation, let\\'s take a trip down memory lane and check out all the fantastic drops they have previously released. Discover YSL BeautÃ© web3 website: https://lnkd.in/g7hsT_Vq',\n",
       " \"What a fantastic week at NFT Paris ! ğŸ¥‚ Met so many incredible people in the  space and I spent some quality time with team members I don't often see face-to-face, like my CTO, Alexandre Cognard ! It was great sharing memories and talking about the progress made since I started at Arianee almost 2 years ago. Learning from him has been an incredible opportunity for me to develop my career and gain insight into other fields such as tech & management. Getting to know the ins and outs of their day-to-day work made me understand the tech team better. I gained insights into their viewpoints, strengthening mutual understanding & boosting alignment within the company. Thanks for the organization Alexandre Tsydenkov ğŸ™Œ Can't wait for next year !\",\n",
       " \"Great coverage by Louise Laing of what's happening in Digital Fashion from NFT Paris. User-Generated Content is taking off and The Sandbox stands as a transformative force in the digital fashion landscape. â€œThe next big designer will emerge from a platform like The Sandbox with their customers recognising them as the next big brand.â€ This is validated by initiatives like the Art of Runway and collaborations with Digital Fashion Week 3D designers, known as voxel creators, can present their creations for sale in the game, gaining exposure to a different audience and garnering the opportunity to monetise from their creations whilst growing their digital community. https://lnkd.in/eHxX2UXZ @TrueStarsMedia @nft_paris @DgtlFashionWeek\",\n",
       " 'ğŸ“Š  Fintech Focus ğŸ”  ğŸ“¢  NFTs - Are Non-Fungible Tokens The Next Big Thing  âŒ›ï¸ or Just Hype? Read my latest post  ğŸ‘‡ to learn more and share your thoughts/experiences/ideas!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(posts)):\n",
    "    posts[i] = remove_hashtags(posts[i])\n",
    "    if posts[i] == '':\n",
    "        posts[i] = 'This is a neutral comment'\n",
    "\n",
    "posts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  Last week I had score:  0.9998553991317749 prediction:  POSITIVE\n",
      "content:  Hello everyone  score:  0.9989281296730042 prediction:  POSITIVE\n",
      "content:  The digital wor score:  0.998602569103241 prediction:  POSITIVE\n",
      "content:  Canâ€™t wait to w score:  0.999728262424469 prediction:  POSITIVE\n",
      "content:  DualMint is set score:  0.9997445940971375 prediction:  POSITIVE\n",
      "content:  This Week on RW score:  0.9849713444709778 prediction:  POSITIVE\n",
      "content:  Join me Saturda score:  0.973907470703125 prediction:  POSITIVE\n",
      "content:  NFT Paris 2024  score:  0.9457706212997437 prediction:  POSITIVE\n",
      "content:  Hello everyone  score:  0.9994600415229797 prediction:  POSITIVE\n",
      "content:  â€œComment achete score:  0.907588005065918 prediction:  POSITIVE\n",
      "content:  A guide to   an score:  0.9981526732444763 prediction:  POSITIVE\n",
      "content:  For a long time score:  0.9995279312133789 prediction:  POSITIVE\n",
      "content:  Brands can now  score:  0.9996111989021301 prediction:  POSITIVE\n",
      "content:  neutral score:  0.9933512806892395 prediction:  NEGATIVE\n",
      "content:  A bit more than score:  0.9999492168426514 prediction:  POSITIVE\n",
      "content:  A guide to Bore score:  0.9878835082054138 prediction:  POSITIVE\n",
      "content:  A SaaS platform score:  0.9994901418685913 prediction:  POSITIVE\n",
      "content:  NFTs are revolu score:  0.7572421431541443 prediction:  POSITIVE\n",
      "content:  Is this the bes score:  0.9909084439277649 prediction:  POSITIVE\n",
      "content:  In just two day score:  0.995640754699707 prediction:  POSITIVE\n",
      "content:  What a fantasti score:  0.9997329115867615 prediction:  POSITIVE\n",
      "content:  Great coverage  score:  0.9989439845085144 prediction:  POSITIVE\n",
      "content:  ğŸ“Š  Fintech Focu score:  0.9314771890640259 prediction:  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['content', 'score', 'prediction'])\n",
    "\n",
    "# Assuming 'model' is defined somewhere in your code\n",
    "# model = ...\n",
    "\n",
    "for i in range(len(posts)):\n",
    "    sentence = flair.data.Sentence(posts[i])\n",
    "    model.predict(sentence)\n",
    "    \n",
    "    # Extract relevant information from sentence and append to the DataFrame\n",
    "    content = posts[i]\n",
    "    score = sentence.labels[0].score\n",
    "    prediction = sentence.labels[0].value\n",
    "    \n",
    "    print('content: ', content[:15], 'score: ', score, 'prediction: ', prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
