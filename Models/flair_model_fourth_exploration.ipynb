{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair uses PyTorch/TensorFlow in under the hood, so it's essential that you also have one of the two libraries (or both) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "#english language model for sentiment analysis\n",
    "model = flair.models.TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to tokenize input text. For this we use the Flair Sentence object, which we initialize by passing our text into it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with `nft` related posts extracted from the crawler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that removing the hashtags definitely improved the prediction. So now, let's test it with more post contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = ['Last week I pleasure to NFT Paris by hand of Arianee Witnessing transformative power of Web NFTs in reshaping concept of ownership particularly in relation to personal data left inspired eager to delve deeper into evolving landscape of decentralized technologies The event sparked valuable insights I excited to continue navigating dynamic realm of blockchain innovations',\n",
    " 'Hello everyone Today friends Asma Ghamacha Hermes Yan NTJAM NDJENG Harold Geumtcheng Aloys Aymrick Nzooh Bryan Fozame I chance to part of NFT Paris conference thanks to school aivancity School for Technology Business Society Paris Cachan learned wealth of new information about blockchain metaverse web use cases across various industries finance gaming luxury During enI privilege to engage in discussions with numerous brilliant web developers CEOs from companies like Maxence Perray from Nomiks Victor Briere from Arianee Ubisoft Louis Vuitton many others These conversations provided with deeper insights into innovative technology intersection with data science particularly in terms of transparency tokenization on blockchain',\n",
    " 'The digital world continues to intersect with traditional art forms online trading platform Robinhood partners with Notable art to bring prominent artist Hunt Slonem work to wider audience through use of Non Fungible Tokens NFTs Sign up on website to automatically enter monthly prize raffle https lnkd in dMcKcrpf',\n",
    " 'Can wait to welcome everyone in Paris week for NFT Paris Shout to Alexandre Tsydenkov team at NFT Paris done awesome job creating curating one of relevant B B B C web events in world Arianee I present throughout week at side events panels of course main event on rd th with demos workshop This year Arianee taking different approach with new activations from taking over VIP lounge exhibiting brands building on technology from BREITLING to Panerai Moncler many showcasing tokenized digital product passports Join on stage on February rd for The opening keynote From Hype to Purpose Redefining NFTs for next billion users at CET A panel with ian rogers Chief Experience Officer Ledger Gmoney on Unleashing Potential of Digital Luxury Market at CET Our fellow team members esteemed partners also on stage Delphine Edde CMO moderating panel on Digital Product Passports for Physical Goods From Post Purchase Engagement to Circular Business Models with Eva Assayag Head of IS Organization Projects Panerai Adrian Corsin Managing Director MUGLER Michele Lo Forte Global Head of E Commerce Digital Customer Engagement BREITLING on Friday rd at CET Alexandre Mare joining Fabien Aufrechter Head of Web Vivendi Sandy Carter COO Unstoppable Domains moderated by Farokh Sarmad Rug Media to discuss Onboarding Next Billions Users into Web Use Cases Challenges Opportunities on Saturday th at CET Our Lead Developer Maxime Vaullerin host workshop on Enhance Digital Product Passport Utilities with Interoperability on Saturday th at pm CET Last least co hosting Speakers Dinner on Thursday nd lunch with Polygon Labs on rd in VIP Lounge If time go check out Musee Orsay dear friend Agoria amazing musician NFT artist opened exhibition with two extraordinary works created specifically for museum Go check out I might organizing little private tour DM interested It going to crazy week looking forward to seeing Thanks to amazing Arianee team for making happen Don hesitate to drop note interested in setting up meeting want to attend of side events need food tips for Paris Click to get full Arianee agenda https lnkd in euJePd D Save Date February Location Grand Palais Ephemere Paris',\n",
    " 'DualMint set to launch Toji NFT Japanese sake tokenized on blockchain on nd March It in collaboration with year old sake producers Daimon Brewery This partnership big deal showcases real world assets tokenization apart from existing real estate tokenization financial product tokenization Want to part of launch Read in latest newsletter This Week on RWA Insights Dive into Day by Day leverages RWAs AI to redefine insurance Get ready for DualMint Toji NFT launch on March nd Explore featured blog post on revolutionizing commodities market Don miss out on groundbreaking insights',\n",
    " 'Join Saturday February th at NFT Paris I speaking alongside Fabien Aufrechter Head of Web Vivendi Sandy Carter COO Unstoppable Domains moderated by Farokh Sarmad Rug Media to discuss Onboarding Next Billions Users into Web Use cases Challenges Opportunities Discover Arianee digital product passports unlock new circular economy Arianee also massive presence at NFT Paris full of panels workshops demo See full agenda below Save Date February Location Grand Palais Ephemere Paris NFT Paris starts in exactly one week Besides taking over VIP Lounge couple things up sleeve Catch out keynote panels workshop look out for Arianee team hint might wearing something pink especially in Builder Zone booth Check out details below save share post see next week Day February rd Opening keynote From Hype to Purpose Redefining NFTs for The Next Billion Users Pierre Nicolas Hurstel CEO Co Founder Arianee Main Stage Panel Digital Product Passports for Physical Goods From Post Purchase Engagement to Circular Business Models Eva Assayag Head of IS Organization Projects Panerai Adrian Corsin Managing Director MUGLER Michele Lo Forte Global Head of E Commerce Digital Customer Engagement BREITLING Moderated by Delphine Edde CMO Arianee pm Main Stage Panel Unleashing Potential of Digital Luxury Market ian rogers Chief Experience Officer Ledger Gmoney Pierre Nicolas CEO Co Founder Arianee Moderated by Amanda Cassatt u b u b CEO Founder Serotonin Main Stage Day February th Panel Onboarding The Next Billions Users into Web Use Cases Challenges Opportunities Fabien Aufrechter Head of Web Vivendi Sandy Carter COO Unstoppable Domains Alexandre Mare COO Arianee Moderated by Farokh Sarmad Rug Radio pm Main Stage Workshop Enhance Digital Product Passport Utilities with Interoperability The Arianee Case Maxime Vaullerin Lead Developer Arianee pm pm Eiffel Stage Look out for announcements next week Book meeting with us in advance https lnkd in guSVreC Still secured tickets Follow link below get special discount with code PN https lnkd in dn D xHR Discover on digital product passport solutions https lnkd in e hD Save Date February Location Grand Palais Ephemere Paris See',\n",
    " 'Hello everyone Today I chance to part of NFT Paris conference I learned wealth of new information about blockchain use cases across various industries finance gaming luxury I privilege to engage in discussions with numerous brilliant web developers CEOs from companies like Maxence Perray from Victor Briere from many others These conversations provided with deeper insights into innovative technology intersection with data science particularly in terms of transparency tokenization on blockchain',\n",
    " 'Comment acheter les NFT de Eyes of Humanity et en tirer profit',\n",
    " 'A guide to Decentraland NFTs snapshot of data trends',\n",
    " 'For long time I imagined term always something Well regardless of circumstances I said waiting enough nothing needs to done for to happen Thus I publicly presented alter ego The Knight with Rose https lnkd in gDWS mas music group MonteCristo https lnkd in g p V KQ first NFT of book combined into NIGHT WITHOUT GAME event With exactly I life born made sense Art lecture conversation music masks fallen games gone We ones left watch with hearts',\n",
    " 'Brands create digital narratives customers actively participate adding immersive dimension to marketing efforts Imagine launching product line item accompanied by NFT telling different story of brand heritage product journey This approach enriches customer experience also fosters deeper connection with brand',\n",
    " 'A bit entries over days NFT Paris blast Particularly proud of attendance of major companies in luxury sport gaming finance art sector support year Even Tesla NFTs evolving sector becoming mature From Hype to purpose introduction keynote from Pierre Nicolas Hurstel defining theme of edition Once hype gone remains A brilliant community culture Real use cases finally appear Slowly at let consolidation work',\n",
    " 'A guide to Bored Ape Yacht Club NFTs snapshot of data trends',\n",
    " 'A SaaS platform on top of Arianee Protocol to distribute gasless NFTs with special features engage customers securely anonymously at scale NFTs revolutionizing way think about ownership value in digital world in ways deeper meaningful customer email address But with nascent technology lot of complexity to navigate That NFT Management Platform NMP comes in to make web accessible to brands In article explore created NMP future envision for',\n",
    " 'Is best way to capitalize on Web conference Some context At end of last year decided with BGA team to double down on effort to provide opportunity for BGA members Web Gaming enthusiasts to network learn from We want BGAConnects to embodiment of new comitment During NFT Paris happy to see different verticals of BGAConnects made sense for every atendee Learn from experience of innovating We fortunate to Yat Siu Nicolas Gilot among many brilliant entrepreneurs in panels Give opportunity for start ups to meet potential investors Just Play games meet visionaries behing One last note Always try to contribute to events based on expertise could provide network of professionals in specific ecosystem Super happy members chance to play role in of best side events of NFT Paris See takeaways https lnkd in eGHhc',\n",
    " 'In two days YSL Beaute reveal latest NFT chapter The Night Is Ours As wait in anticipation let take trip down memory lane check out fantastic drops previously released Discover YSL Beaute web website https lnkd in g hsT Vq',\n",
    " 'What fantastic week at NFT Paris Met many incredible people in Web space I spent quality time with team members I often see face to face like CTO Alexandre Cognard It great sharing memories talking about progress made since I started at Arianee almost years ago Learning from incredible opportunity for to develop career gain insight into fields tech management Getting to know ins outs of day to day work made understand tech team better I gained insights into viewpoints strengthening mutual understanding boosting alignment within company Thanks for organization Alexandre Tsydenkov Can wait for next year',\n",
    " 'Great coverage by Louise Laing of happening in Digital Fashion from NFT Paris User Generated Content taking off The Sandbox stands transformative force in digital fashion landscape The next big designer emerge from platform like The Sandbox with customers recognising next big brand This validated by initiatives like Art of Runway collaborations with Digital Fashion Week D designers known voxel creators present creations for sale in game gaining exposure to different audience garnering opportunity to monetise from creations whilst growing digital community https lnkd in eHxX UXZ TrueStarsMedia nft paris DgtlFashionWeek',\n",
    " 'Fintech Focus NFTs Are Non Fungible Tokens The Next Big Thing Just Hype Read latest post to learn share thoughts experiences ideas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  Last week I ple score:  0.9997920393943787 prediction:  POSITIVE\n",
      "content:  Hello everyone  score:  0.998921275138855 prediction:  POSITIVE\n",
      "content:  The digital wor score:  0.9973558187484741 prediction:  POSITIVE\n",
      "content:  Can wait to wel score:  0.999750554561615 prediction:  POSITIVE\n",
      "content:  DualMint set to score:  0.9980905652046204 prediction:  POSITIVE\n",
      "content:  Join Saturday F score:  0.9879761338233948 prediction:  POSITIVE\n",
      "content:  Hello everyone  score:  0.9975327253341675 prediction:  POSITIVE\n",
      "content:  Comment acheter score:  0.9912103414535522 prediction:  POSITIVE\n",
      "content:  A guide to Dece score:  0.9957720637321472 prediction:  POSITIVE\n",
      "content:  For long time I score:  0.9983103275299072 prediction:  POSITIVE\n",
      "content:  Brands create d score:  0.9984293580055237 prediction:  POSITIVE\n",
      "content:  A bit entries o score:  0.9958184361457825 prediction:  POSITIVE\n",
      "content:  A guide to Bore score:  0.9737816452980042 prediction:  POSITIVE\n",
      "content:  A SaaS platform score:  0.9965752959251404 prediction:  POSITIVE\n",
      "content:  Is best way to  score:  0.8451470136642456 prediction:  NEGATIVE\n",
      "content:  In two days YSL score:  0.9925082325935364 prediction:  POSITIVE\n",
      "content:  What fantastic  score:  0.9985466599464417 prediction:  POSITIVE\n",
      "content:  Great coverage  score:  0.9970968961715698 prediction:  POSITIVE\n",
      "content:  Fintech Focus N score:  0.9997010827064514 prediction:  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['content', 'score', 'prediction'])\n",
    "\n",
    "for i in range(len(posts)):\n",
    "    sentence = flair.data.Sentence(posts[i])\n",
    "    model.predict(sentence)\n",
    "    \n",
    "    # Extract relevant information from sentence and append to the DataFrame\n",
    "    content = posts[i]\n",
    "    score = sentence.labels[0].score\n",
    "    prediction = sentence.labels[0].value\n",
    "    \n",
    "    print('content: ', content[:15], 'score: ', score, 'prediction: ', prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stopwords vs leaving stopwords\n",
    "\n",
    "In this exploration we remove the stopwords but left all prepositons, now we can see the previous example of *'A guide to Bored Ape Yacht Club #NFTs and a snapshot of data trends.'* is predicted correctly. However, the case:\n",
    "\n",
    "- content:  Is best way to  score:  0.8451470136642456 prediction:  NEGATIVE\n",
    "\n",
    "Is in incorrect, it should be calculated as POSITIVE therefore we can say the 1/19 are predicted incorrectly by this model. An accuracy of 94%.\n",
    "\n",
    "Therefore, the ideal thing to follow is to create my own model from zero and train it for this tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps for Arianee\n",
    " - Get more data through my crawler - basically iterating through more pages\n",
    " - Label the data myself as positive, negative, or neutral\n",
    " - Train a new transformer model with more data\n",
    "\n",
    "https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 16:34:57,157 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to C:\\Users\\matts\\AppData\\Local\\Temp\\tmpq3n1od6r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2.99M/153M [00:13<09:38, 271kB/s]   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mWordEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglove\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, FlairEmbeddings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews-forward-fast\u001b[39m\u001b[38;5;124m'\u001b[39m), FlairEmbeddings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews-backward-fast\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      3\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m DocumentLSTMEmbeddings(word_embeddings, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, reproject_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reproject_words_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#substitute for my corpus using NLPTaskDataFetcher\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\embeddings\\token.py:184\u001b[0m, in \u001b[0;36mWordEmbeddings.__init__\u001b[1;34m(self, embeddings, field, fine_tune, force_cpu, stable, no_header, vocab, embedding_length, name)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fine_tune \u001b[38;5;129;01mand\u001b[39;00m force_cpu \u001b[38;5;129;01mand\u001b[39;00m flair\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot train WordEmbeddings on cpu if the model is trained on gpu, set force_cpu=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 184\u001b[0m embeddings_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve_precomputed_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(embeddings_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\embeddings\\token.py:251\u001b[0m, in \u001b[0;36mWordEmbeddings.resolve_precomputed_path\u001b[1;34m(self, embeddings)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# GLOVE embeddings\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-glove\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhu_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/glove.gensim.vectors.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhu_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/glove.gensim\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# TURIAN embeddings\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\file_utils.py:109\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[0;32m    105\u001b[0m parsed \u001b[38;5;241m=\u001b[39m urlparse(url_or_filename)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m download_s3_to_path(parsed\u001b[38;5;241m.\u001b[39mnetloc, dataset_cache)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\file_utils.py:229\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir)\u001b[0m\n\u001b[0;32m    227\u001b[0m progress \u001b[38;5;241m=\u001b[39m Tqdm\u001b[38;5;241m.\u001b[39mtqdm(unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mtotal, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(temp_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m req\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    231\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3.08M/153M [00:30<09:37, 271kB/s]"
     ]
    }
   ],
   "source": [
    "#corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "#substitute for my corpus using NLPTaskDataFetcher\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
