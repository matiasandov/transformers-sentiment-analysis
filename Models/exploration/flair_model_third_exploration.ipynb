{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair uses PyTorch/TensorFlow in under the hood, so it's essential that you also have one of the two libraries (or both) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "#english language model for sentiment analysis\n",
    "model = flair.models.TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to tokenize input text. For this we use the Flair Sentence object, which we initialize by passing our text into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[7]: \"I like you. I love you\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I like you. I love you\"  # we are expecting a confidently positive sentiment here\n",
    "\n",
    "sentence = flair.data.Sentence(text)\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[7]: \"I like you. I love you\" → POSITIVE (0.9933)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sentence)\n",
    "# The predict method doesn't output our prediction, instead the predictions are added to our sentence:\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence[7]: \"I like you. I love you\"'/'POSITIVE' (0.9933)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.get_labels()[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with `nft` related posts extracted from the crawler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that removing the hashtags definitely improved the prediction. So now, let's test it with more post contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = ['Last week I pleasure NFT Paris hand Arianee Witnessing transformative power Web NFTs reshaping concept ownership particularly relation personal data left inspired eager delve deeper evolving landscape decentralized technologies The event sparked valuable insights I excited continue navigating dynamic realm blockchain innovations',\n",
    " 'Hello everyone Today friends Asma Ghamacha Hermes Yan NTJAM NDJENG Harold Geumtcheng Aloys Aymrick Nzooh Bryan Fozame I chance part NFT Paris conference thanks school aivancity School Technology Business Society Paris Cachan learned wealth new information blockchain metaverse web use cases across various industries finance gaming luxury During enI privilege engage discussions numerous brilliant web developers CEOs companies like Maxence Perray Nomiks Victor Briere Arianee Ubisoft Louis Vuitton many others These conversations provided deeper insights innovative technology intersection data science particularly terms transparency tokenization blockchain',\n",
    " 'The digital world continues intersect traditional art forms online trading platform Robinhood partners Notable art bring prominent artist Hunt Slonem work wider audience use Non Fungible Tokens NFTs Sign website automatically enter monthly prize raffle https lnkd dMcKcrpf',\n",
    " 'Can wait welcome everyone Paris week NFT Paris Shout Alexandre Tsydenkov team NFT Paris done awesome job creating curating one relevant B B B C web events world Arianee I present throughout week side events panels course main event rd th demos workshop This year Arianee taking different approach new activations taking VIP lounge exhibiting brands building technology BREITLING Panerai Moncler many showcasing tokenized digital product passports Join stage February rd The opening keynote From Hype Purpose Redefining NFTs next billion users CET A panel ian rogers Chief Experience Officer Ledger Gmoney Unleashing Potential Digital Luxury Market CET Our fellow team members esteemed partners also stage Delphine Edde CMO moderating panel Digital Product Passports Physical Goods From Post Purchase Engagement Circular Business Models Eva Assayag Head IS Organization Projects Panerai Adrian Corsin Managing Director MUGLER Michele Lo Forte Global Head E Commerce Digital Customer Engagement BREITLING Friday rd CET Alexandre Mare joining Fabien Aufrechter Head Web Vivendi Sandy Carter COO Unstoppable Domains moderated Farokh Sarmad Rug Media discuss Onboarding Next Billions Users Web Use Cases Challenges Opportunities Saturday th CET Our Lead Developer Maxime Vaullerin host workshop Enhance Digital Product Passport Utilities Interoperability Saturday th pm CET Last least co hosting Speakers Dinner Thursday nd lunch Polygon Labs rd VIP Lounge If time go check Musee Orsay dear friend Agoria amazing musician NFT artist opened exhibition two extraordinary works created specifically museum Go check I might organizing little private tour DM interested It going crazy week looking forward seeing Thanks amazing Arianee team making happen Don hesitate drop note interested setting meeting want attend side events need food tips Paris Click get full Arianee agenda https lnkd euJePd D Save Date February Location Grand Palais Ephemere Paris',\n",
    " 'DualMint set launch Toji NFT Japanese sake tokenized blockchain nd March It collaboration year old sake producers Daimon Brewery This partnership big deal showcases real world assets tokenization apart existing real estate tokenization financial product tokenization Want part launch Read latest newsletter This Week RWA Insights Dive Day Day leverages RWAs AI redefine insurance Get ready DualMint Toji NFT launch March nd Explore featured blog post revolutionizing commodities market Don miss groundbreaking insights',\n",
    " 'Join Saturday February th NFT Paris I speaking alongside Fabien Aufrechter Head Web Vivendi Sandy Carter COO Unstoppable Domains moderated Farokh Sarmad Rug Media discuss Onboarding Next Billions Users Web Use cases Challenges Opportunities Discover Arianee digital product passports unlock new circular economy Arianee also massive presence NFT Paris full panels workshops demo See full agenda Save Date February Location Grand Palais Ephemere Paris NFT Paris starts exactly one week Besides taking VIP Lounge couple things sleeve Catch keynote panels workshop look Arianee team hint might wearing something pink especially Builder Zone booth Check details save share post see next week Day February rd Opening keynote From Hype Purpose Redefining NFTs The Next Billion Users Pierre Nicolas Hurstel CEO Co Founder Arianee Main Stage Panel Digital Product Passports Physical Goods From Post Purchase Engagement Circular Business Models Eva Assayag Head IS Organization Projects Panerai Adrian Corsin Managing Director MUGLER Michele Lo Forte Global Head E Commerce Digital Customer Engagement BREITLING Moderated Delphine Edde CMO Arianee pm Main Stage Panel Unleashing Potential Digital Luxury Market ian rogers Chief Experience Officer Ledger Gmoney Pierre Nicolas CEO Co Founder Arianee Moderated Amanda Cassatt u b u b CEO Founder Serotonin Main Stage Day February th Panel Onboarding The Next Billions Users Web Use Cases Challenges Opportunities Fabien Aufrechter Head Web Vivendi Sandy Carter COO Unstoppable Domains Alexandre Mare COO Arianee Moderated Farokh Sarmad Rug Radio pm Main Stage Workshop Enhance Digital Product Passport Utilities Interoperability The Arianee Case Maxime Vaullerin Lead Developer Arianee pm pm Eiffel Stage Look announcements next week Book meeting us advance https lnkd guSVreC Still secured tickets Follow link get special discount code PN https lnkd dn D xHR Discover digital product passport solutions https lnkd e hD Save Date February Location Grand Palais Ephemere Paris See',\n",
    " 'Hello everyone Today I chance part NFT Paris conference I learned wealth new information blockchain use cases across various industries finance gaming luxury I privilege engage discussions numerous brilliant web developers CEOs companies like Maxence Perray Victor Briere many others These conversations provided deeper insights innovative technology intersection data science particularly terms transparency tokenization blockchain',\n",
    " 'Comment acheter les NFT de Eyes Humanity et en tirer profit',\n",
    " 'A guide Decentraland NFTs snapshot data trends',\n",
    " 'For long time I imagined term always something Well regardless circumstances I said waiting enough nothing needs done happen Thus I publicly presented alter ego The Knight Rose https lnkd gDWS mas music group MonteCristo https lnkd g p V KQ first NFT book combined NIGHT WITHOUT GAME event With exactly I life born made sense Art lecture conversation music masks fallen games gone We ones left watch hearts',\n",
    " 'Brands create digital narratives customers actively participate adding immersive dimension marketing efforts Imagine launching product line item accompanied NFT telling different story brand heritage product journey This approach enriches customer experience also fosters deeper connection brand',\n",
    " 'A bit entries days NFT Paris blast Particularly proud attendance major companies luxury sport gaming finance art sector support year Even Tesla NFTs evolving sector becoming mature From Hype purpose introduction keynote Pierre Nicolas Hurstel defining theme edition Once hype gone remains A brilliant community culture Real use cases finally appear Slowly let consolidation work',\n",
    " 'A guide Bored Ape Yacht Club NFTs snapshot data trends',\n",
    " 'A SaaS platform top Arianee Protocol distribute gasless NFTs special features engage customers securely anonymously scale NFTs revolutionizing way think ownership value digital world ways deeper meaningful customer email address But nascent technology lot complexity navigate That NFT Management Platform NMP comes make web accessible brands In article explore created NMP future envision',\n",
    " 'Is best way capitalize Web conference Some context At end last year decided BGA team double effort provide opportunity BGA members Web Gaming enthusiasts network learn We want BGAConnects embodiment new comitment During NFT Paris happy see different verticals BGAConnects made sense every atendee Learn experience innovating We fortunate Yat Siu Nicolas Gilot among many brilliant entrepreneurs panels Give opportunity start ups meet potential investors Just Play games meet visionaries behing One last note Always try contribute events based expertise could provide network professionals specific ecosystem Super happy members chance play role best side events NFT Paris See takeaways https lnkd eGHhc',\n",
    " 'In two days YSL Beaute reveal latest NFT chapter The Night Is Ours As wait anticipation let take trip memory lane check fantastic drops previously released Discover YSL Beaute web website https lnkd g hsT Vq',\n",
    " 'What fantastic week NFT Paris Met many incredible people Web space I spent quality time team members I often see face face like CTO Alexandre Cognard It great sharing memories talking progress made since I started Arianee almost years ago Learning incredible opportunity develop career gain insight fields tech management Getting know ins outs day day work made understand tech team better I gained insights viewpoints strengthening mutual understanding boosting alignment within company Thanks organization Alexandre Tsydenkov Can wait next year',\n",
    " 'Great coverage Louise Laing happening Digital Fashion NFT Paris User Generated Content taking The Sandbox stands transformative force digital fashion landscape The next big designer emerge platform like The Sandbox customers recognising next big brand This validated initiatives like Art Runway collaborations Digital Fashion Week D designers known voxel creators present creations sale game gaining exposure different audience garnering opportunity monetise creations whilst growing digital community https lnkd eHxX UXZ TrueStarsMedia nft paris DgtlFashionWeek',\n",
    " 'Fintech Focus NFTs Are Non Fungible Tokens The Next Big Thing Just Hype Read latest post learn share thoughts experiences ideas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  Last week I ple score:  0.9994392991065979 prediction:  POSITIVE\n",
      "content:  Hello everyone  score:  0.9984716773033142 prediction:  POSITIVE\n",
      "content:  The digital wor score:  0.9924715757369995 prediction:  POSITIVE\n",
      "content:  Can wait welcom score:  0.9996523857116699 prediction:  POSITIVE\n",
      "content:  DualMint set la score:  0.9991014003753662 prediction:  POSITIVE\n",
      "content:  Join Saturday F score:  0.9017152190208435 prediction:  POSITIVE\n",
      "content:  Hello everyone  score:  0.9986838698387146 prediction:  POSITIVE\n",
      "content:  Comment acheter score:  0.9600447416305542 prediction:  POSITIVE\n",
      "content:  A guide Decentr score:  0.9980354905128479 prediction:  POSITIVE\n",
      "content:  For long time I score:  0.9851458072662354 prediction:  POSITIVE\n",
      "content:  Brands create d score:  0.9983072280883789 prediction:  POSITIVE\n",
      "content:  A bit entries d score:  0.9938095211982727 prediction:  POSITIVE\n",
      "content:  A guide Bored A score:  0.765382707118988 prediction:  NEGATIVE\n",
      "content:  A SaaS platform score:  0.992318868637085 prediction:  POSITIVE\n",
      "content:  Is best way cap score:  0.9509221315383911 prediction:  POSITIVE\n",
      "content:  In two days YSL score:  0.8887806534767151 prediction:  POSITIVE\n",
      "content:  What fantastic  score:  0.9981697797775269 prediction:  POSITIVE\n",
      "content:  Great coverage  score:  0.9956810474395752 prediction:  POSITIVE\n",
      "content:  Fintech Focus N score:  0.9996289014816284 prediction:  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['content', 'score', 'prediction'])\n",
    "\n",
    "for i in range(len(posts)):\n",
    "    sentence = flair.data.Sentence(posts[i])\n",
    "    model.predict(sentence)\n",
    "    \n",
    "    # Extract relevant information from sentence and append to the DataFrame\n",
    "    content = posts[i]\n",
    "    score = sentence.labels[0].score\n",
    "    prediction = sentence.labels[0].value\n",
    "    \n",
    "    print('content: ', content[:15], 'score: ', score, 'prediction: ', prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stopwords vs leaving stopwords\n",
    "\n",
    "We have a very particular case here where I originally have a post that says:\n",
    "\n",
    "\"*A guide to Bored Ape Yacht Club #NFTs and a snapshot of data trends*\"\n",
    "\n",
    "Nevertheless if we transform this description with the preprocessing methods in 'nlp_preprocessing.ipynb' and 'preprocessing_no_lemm.ipynb' the results would be respectively:\n",
    "\n",
    "1.  'guide bored yacht club nfts snapshot data trend',\n",
    "2. 'A guide Bored Ape Yacht Club NFTs snapshot data trends'\n",
    "\n",
    "Which is predicted **negative** by the flair model, there I tried experimenting by adding back the stopword \"*to*\" to observe if it can make a difference. Therefore the results were the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  Span[0:7]: \"A guide Bored Ape Yacht Club NFTs\" score:  0.765382707118988 prediction:  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "sentence = flair.data.Sentence( 'A guide Bored Ape Yacht Club NFTs snapshot data trends')\n",
    "model.predict(sentence)\n",
    "score = sentence.labels[0].score\n",
    "prediction = sentence.labels[0].value\n",
    "\n",
    "print('content: ', sentence[:7], 'score: ', score, 'prediction: ', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  Span[0:7]: \"A guide to bored ape yacht club\" score:  0.9295165538787842 prediction:  POSITIVE\n"
     ]
    }
   ],
   "source": [
    "sentence = flair.data.Sentence( 'A guide to bored ape yacht club NFTs snapshot data trends')\n",
    "model.predict(sentence)\n",
    "score = sentence.labels[0].score\n",
    "prediction = sentence.labels[0].value\n",
    "\n",
    "print('content: ', sentence[:7], 'score: ', score, 'prediction: ', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  Span[0:7]: \"My baby looks like a little bear\" score:  0.6381192207336426 prediction:  POSITIVE\n"
     ]
    }
   ],
   "source": [
    "sentence = flair.data.Sentence( 'My baby looks like a little bear')\n",
    "model.predict(sentence)\n",
    "score = sentence.labels[0].score\n",
    "prediction = sentence.labels[0].value\n",
    "\n",
    "print('content: ', sentence[:7], 'score: ', score, 'prediction: ', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that by adding the stopword *to* the prediction turns out to be positive, which is better than being predicted as negative, therefore, it could be better to not remove prepostions or just the stopword *to*, or consider not removing any stopword at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps for Arianee\n",
    " - Get more data through my crawler - basically iterating through more pages\n",
    " - Label the data myself as positive, negative, or neutral\n",
    " - Train a new transformer model with more data\n",
    "\n",
    "https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:17:22,949 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to C:\\Users\\matts\\AppData\\Local\\Temp\\tmpudqkhddm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 13.4M/153M [00:42<13:57, 174kB/s]   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mWordEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglove\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, FlairEmbeddings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews-forward-fast\u001b[39m\u001b[38;5;124m'\u001b[39m), FlairEmbeddings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews-backward-fast\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      3\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m DocumentLSTMEmbeddings(word_embeddings, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, reproject_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reproject_words_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#substitute for my corpus using NLPTaskDataFetcher\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\embeddings\\token.py:184\u001b[0m, in \u001b[0;36mWordEmbeddings.__init__\u001b[1;34m(self, embeddings, field, fine_tune, force_cpu, stable, no_header, vocab, embedding_length, name)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fine_tune \u001b[38;5;129;01mand\u001b[39;00m force_cpu \u001b[38;5;129;01mand\u001b[39;00m flair\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot train WordEmbeddings on cpu if the model is trained on gpu, set force_cpu=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 184\u001b[0m embeddings_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve_precomputed_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(embeddings_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\embeddings\\token.py:251\u001b[0m, in \u001b[0;36mWordEmbeddings.resolve_precomputed_path\u001b[1;34m(self, embeddings)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# GLOVE embeddings\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-glove\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhu_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/glove.gensim.vectors.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhu_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/glove.gensim\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# TURIAN embeddings\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\file_utils.py:109\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[0;32m    105\u001b[0m parsed \u001b[38;5;241m=\u001b[39m urlparse(url_or_filename)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m download_s3_to_path(parsed\u001b[38;5;241m.\u001b[39mnetloc, dataset_cache)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\file_utils.py:229\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir)\u001b[0m\n\u001b[0;32m    227\u001b[0m progress \u001b[38;5;241m=\u001b[39m Tqdm\u001b[38;5;241m.\u001b[39mtqdm(unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mtotal, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(temp_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m req\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    231\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 13.4M/153M [01:00<13:57, 174kB/s]"
     ]
    }
   ],
   "source": [
    "#corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "#substitute for my corpus using NLPTaskDataFetcher\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
